# Flask / app settings
FLASK_ENV=development
FLASK_SECRET_KEY=change-me

# Paths
DATABASE_PATH=instance/app.db
PAPERS_DATA_DIR=CodeArXiv-data

# Auth
NO_AUTH_MODE=true
DEFAULT_USER_USERNAME=guest
DEFAULT_USER_PASSWORD=guest

# LLM processing (run_daily.py / run_papers_cool.py / codex_fill_zh.py)
# Options: codex (Codex CLI) | api (OpenAI-compatible HTTP API)
LLM_PROVIDER=codex
CODEX_MODEL=
CODEX_REASONING_EFFORT=low
CODEX_REASONING_SUMMARY=concise
CODEX_BATCH_SIZE=5
CODEX_TIMEOUT=300
CODEX_SLEEP=0.2
CODEX_OVERWRITE=false

# Standard OpenAI-compatible API backend (codex_fill_zh.py --backend api)
# - Enable by setting: LLM_PROVIDER=api
# - Do NOT commit your key; keep it only in your local .env (this file is an example).
LLM_BASE_URL=
LLM_API_KEY=
LLM_MODEL_NAME=
# Optional: increase if you see truncated JSON outputs
LLM_MAX_OUTPUT_TOKENS=
LLM_TEMPERATURE=0
# Optional: dump prompt/raw/meta files for failed LLM calls (helps debug "No JSON object found")
LLM_DEBUG_DUMP_DIR=

# Legacy env var aliases (kept for backward compatibility; prefer LLM_* above)
KIMI_API_KEY=
KIMI_BASE_URL=
KIMI_MODEL=
KIMI_MAX_OUTPUT_TOKENS=
KIMI_TEMPERATURE=
